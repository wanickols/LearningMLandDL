{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUSwof6Qvx7JyEj9bKOPAj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wanickols01/LearningMLandDL/blob/main/SparkTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install pyspark\n",
        "##### Remember to upload subset.small.tsv (in github repo)\n"
      ],
      "metadata": {
        "id": "zfE2HWgZXtdH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE0LpdNwP8s-",
        "outputId": "fa3c4eb2-3053-4408-bb1f-d548814617a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.5)\n",
            "openjdk-8-jdk-headless is already the newest version (8u362-ga-0ubuntu1~20.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import some libraries we'll need "
      ],
      "metadata": {
        "id": "AznDztwYXhAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.mllib.feature import HashingTF\n",
        "from pyspark.mllib.feature import IDF\n"
      ],
      "metadata": {
        "id": "YcC6I5u2Q3jE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the session and context"
      ],
      "metadata": {
        "id": "Ru3olJVrXclP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boilerplate Spark stuff:\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"SparkTFIDF\")\n",
        "sc = SparkContext(conf = conf)"
      ],
      "metadata": {
        "id": "zcbYBmAgQ6Hl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load documents (one per line).\n",
        "rawData = sc.textFile(\"subset-small.tsv\")\n",
        "fields = rawData.map(lambda x: x.split(\"\\t\"))\n",
        "documents = fields.map(lambda x: x[3].split(\" \"))\n",
        "\n",
        "# Store the document names for later:\n",
        "documentNames = fields.map(lambda x: x[1])\n",
        "\n",
        "#Now hash the words in each document to their term frequencies:\n",
        "hashingTF = HashingTF(100000)  #100K hash buckets just to save some memory\n",
        "tf = hashingTF.transform(documents)"
      ],
      "metadata": {
        "id": "eFd5YKsiRFZ_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point we have an RDD of sparse vectors representing each document,\n",
        "where each value maps to the term frequency of each unique hash value.\n"
      ],
      "metadata": {
        "id": "nUBzOzwKXY0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compute the TF*IDF of each term in each document:\n",
        "tf.cache()\n",
        "idf = IDF(minDocFreq=2).fit(tf)\n",
        "tfidf = idf.transform(tf)"
      ],
      "metadata": {
        "id": "CtkBAR1AQxu0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have an RDD of sparse vectors, where each value is the TFxIDF\n",
        "of each unique hash value for each document.\n",
        "\n",
        "I happen to know that the article for \"Abraham Lincoln\" is in our data\n",
        "set, so let's search for \"Gettysburg\" (Lincoln gave a famous speech there):\n",
        "\n",
        "First, let's figure out what hash value \"Gettysburg\" maps to by finding the index a sparse vector from HashingTF gives us back:"
      ],
      "metadata": {
        "id": "G5iKT-8IXQvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gettysburgTF = hashingTF.transform([\"Gettysburg\"])\n",
        "gettysburgHashValue = int(gettysburgTF.indices[0])\n"
      ],
      "metadata": {
        "id": "-vEcZLmRXO6l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will extract the TF*IDF score for Gettsyburg's hash value into"
      ],
      "metadata": {
        "id": "9oNI5zSuZwAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# a new RDD for each document:\n",
        "gettysburgRelevance = tfidf.map(lambda x: x[gettysburgHashValue])\n",
        "\n",
        "# We'll zip in the document names so we can see which is which:\n",
        "zippedResults = gettysburgRelevance.zip(documentNames)"
      ],
      "metadata": {
        "id": "hDqh1oheZYip"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And, print the document with the maximum TF*IDF value:\n",
        "print(\"Best document for Gettysburg is:\")\n",
        "print(zippedResults.max())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJJJzyUsZZ-i",
        "outputId": "58c75054-dedb-43b7-b49c-eaf71e131b0c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best document for Gettysburg is:\n",
            "(8.27233211415088, 'Ares')\n"
          ]
        }
      ]
    }
  ]
}