{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wanickols01/LearningMLandDL/blob/main/Keras_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNeHiRGp2KKM"
      },
      "source": [
        "# Introducing Keras\n",
        "\n",
        "Let's use Keras on the MNIST data set again, this time using a Convolutional Neural Network that's better suited for image processing. CNN's are less sensitive to where in the image the pattern is that we're looking for.\n",
        "\n",
        "With a multi-layer perceptron, we achieved around 97% accuracy. Let's see if we can beat that.\n",
        "\n",
        "As before we'll start by importing the stuff we need, including the new layer types we talked about:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mtq3_de82KKO"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLdPCuGS2KKP"
      },
      "source": [
        "We'll load up our raw data set exactly as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xM391zB2KKP",
        "outputId": "6ee70dd5-61eb-4ad0-e415-6aa363621920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Hj6-wq2KKP"
      },
      "source": [
        "We need to shape the data differently then before. Since we're treating the data as 2D images of 28x28 pixels instead of a flattened stream of 784 pixels, we need to shape it accordingly. Depending on the data format Keras is set up for, this may be 1x28x28 or 28x28x1 (the \"1\" indicates a single color channel, as this is just grayscale. If we were dealing with color images, it would be 3 instead of 1 since we'd have red, green, and blue color channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BvjlbjMX2KKP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
        "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
        "    input_shape = (1, 28, 28)\n",
        "else:\n",
        "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
        "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
        "    input_shape = (28, 28, 1)\n",
        "    \n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "train_images /= 255\n",
        "test_images /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP-s9vaC2KKP"
      },
      "source": [
        "As before we need to convert our train and test labels to be categorical in one-hot format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "trartFIm2KKP"
      },
      "outputs": [],
      "source": [
        "train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
        "test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gcJERkT2KKP"
      },
      "source": [
        "As a sanity check let's print out one of the training images with its label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "eEYIFfsG2KKP",
        "outputId": "722f4bc2-5abe-46b7-b82e-c61791779f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_sample(num):\n",
        "    #Print the one-hot array of this sample's label \n",
        "    print(train_labels[num])  \n",
        "    #Print the label converted back to a number\n",
        "    label = train_labels[num].argmax(axis=0)\n",
        "    #Reshape the 768 values to a 28x28 image\n",
        "    image = train_images[num].reshape([28,28])\n",
        "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
        "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
        "    plt.show()\n",
        "    \n",
        "display_sample(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nB5rRw-2KKQ"
      },
      "source": [
        "Now for the meat of the problem. Setting up a convolutional neural network involves more layers. Not all of these are strictly necessary; you could run without pooling and dropout, but those extra steps help avoid overfitting and help things run faster.\n",
        "\n",
        "We'll start with a 2D convolution of the image - it's set up to take 32 windows, or \"filters\", of each image, each filter being 3x3 in size.\n",
        "\n",
        "We then run a second convolution on top of that with 64 3x3 windows - this topology is just what comes recommended within Keras's own examples. Again you want to re-use previous research whenever possible while tuning CNN's, as it is hard to do.\n",
        "\n",
        "Next we apply a MaxPooling2D layer that takes the maximum of each 2x2 result to distill the results down into something more manageable.\n",
        "\n",
        "A dropout filter is then applied to prevent overfitting.\n",
        "\n",
        "Next we flatten the 2D layer we have at this stage into a 1D layer. So at this point we can just pretend we have a traditional multi-layer perceptron...\n",
        "\n",
        "... and feed that into a hidden, flat layer of 128 units.\n",
        "\n",
        "We then apply dropout again to further prevent overfitting.\n",
        "\n",
        "And finally, we feed that into our final 10 units where softmax is applied to choose our category of 0-9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SbDXIpev2KKR"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "# 64 3x3 kernels\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# Reduce by taking the max of each 2x2 block\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# Dropout to avoid overfitting\n",
        "model.add(Dropout(0.25))\n",
        "# Flatten the results to one dimension for passing into our final layer\n",
        "model.add(Flatten())\n",
        "# A hidden layer to learn with\n",
        "model.add(Dense(128, activation='relu'))\n",
        "# Another dropout\n",
        "model.add(Dropout(0.5))\n",
        "# Final categorization from 0-9 with softmax\n",
        "model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wgNV4us2KKR"
      },
      "source": [
        "Let's double check the model description:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDAhMPcN2KKR",
        "outputId": "e12dd70b-0676-4a01-a4f0-adab19a39c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 12, 12, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 9216)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               1179776   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smde8JHB2KKR"
      },
      "source": [
        "We are still doing multiple categorization, so categorical_crossentropy is still the right loss function to use. We'll use the Adam optimizer, although the example provided with Keras uses RMSProp. You might want to try both if you have time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1cg41DnL2KKR"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcSwS5lk2KKR"
      },
      "source": [
        "And now we train our model... to make things go a little faster, we'll use batches of 32.\n",
        "\n",
        "## Warning\n",
        "\n",
        "This could take hours to run, and your computer's CPU will be maxed out during that time! Don't run the next block unless you can tie up your computer for a long time. It will print progress as each epoch is run, but each epoch can take around 20 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "GNDCWfKx2KKR",
        "outputId": "f08bd915-d866-46af-c5fc-79374305272d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e20172e6e3b6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(train_images, train_labels,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     validation_data=(test_images, test_labels))\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "history = model.fit(train_images, train_labels,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    verbose=2,\n",
        "                    validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WurKUixB2KKR"
      },
      "source": [
        "Was it worth the wait?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jPccZqR2KKR",
        "outputId": "d48ea48c-be10-4bb6-8eb4-807eea3ae86d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.034441117036675316\n",
            "Test accuracy: 0.9908\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpcI__ZS2KKR"
      },
      "source": [
        "Over 99%! And that's with just 10 epochs! And from the looks of it, 4 or 5 would have been enough. It came at a significant cost in terms of computing power, but when you start distributing things over multiple computers each with multiple GPU's, that cost starts to feel less bad. If you're building something where life and death are on the line, like a self-driving car, every fraction of a percent matters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vnm_1SZE2KKR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}